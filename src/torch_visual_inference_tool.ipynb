{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import queue\n",
    "import random\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "from importlib import reload\n",
    "from multiprocessing import Process, Manager\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.nn import Module\n",
    "from torchvision import models, transforms\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "print(\"Starting...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Async Processes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class GPUMonitor(Process):\n",
    "    def __init__(self, delay):\n",
    "        super(GPUMonitor, self).__init__()\n",
    "        self.delay = delay\n",
    "        self.power_readings = Manager().list()\n",
    "        self.utilization_readings = Manager().list()\n",
    "        self.running = True\n",
    "        self.command = 'nvidia-smi --query-gpu=power.draw,utilization.gpu, --format=csv,noheader,nounits'.split(' ')\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        while self.running:\n",
    "            try:\n",
    "                output_bytes = subprocess.check_output(self.command).strip()\n",
    "                output_string = output_bytes.decode('utf-8')\n",
    "                gpu_power, gpu_utilization = output_string.split(',')\n",
    "                self.power_readings.append(float(gpu_power.strip()))\n",
    "                self.utilization_readings.append(float(gpu_utilization.strip()))\n",
    "            except:\n",
    "                logging.error('Something went wrong while retrieving GPU readings...')\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "    def reset_energy(self):\n",
    "        self.power_readings[:] = []\n",
    "        self.utilization_readings[:] = []\n",
    "\n",
    "    def get_power_average(self):\n",
    "        return np.mean(self.power_readings)\n",
    "\n",
    "    def get_utilization_average(self):\n",
    "        return np.mean(self.utilization_readings)\n",
    "\n",
    "    def plot_power(self):\n",
    "        plt.title(\"Power\")\n",
    "        plt.plot(self.power_readings)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_utilization(self):\n",
    "        plt.title(\"Utilization\")\n",
    "        plt.plot(self.utilization_readings)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class RequestQueue(Process):\n",
    "    def __init__(self, id, frequency, nr_of_requests):\n",
    "        super(Process, self).__init__()\n",
    "        self.id = id\n",
    "        self.frequency = frequency\n",
    "        self.nr_of_requests = nr_of_requests\n",
    "        self.queue = Manager().Queue(nr_of_requests)\n",
    "        self.total_time_in_queue = Manager().Value(float, 0.0)\n",
    "        self.max_wait_time_in_queue = Manager().Value(float, 0.0)\n",
    "        self.batch_start_times = Manager().list()\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        logging.info(\"Started simulation with id: {}\".format(self.id))\n",
    "        while self.nr_of_requests > 0:\n",
    "            self.queue.put((random.choice(['img/dog.jpg', 'img/bald_eagle.jpg', 'img/strawberries.jpg']), time.perf_counter()))\n",
    "            self.nr_of_requests -= 1\n",
    "            time.sleep(1 / self.frequency * random.uniform(0.8, 1.2))\n",
    "\n",
    "    def get_request(self, block=True, timeout=None):\n",
    "        img, t_0 = self.queue.get(block, timeout)\n",
    "        self.batch_start_times.append(t_0)\n",
    "        return img\n",
    "\n",
    "    def update_wait_times(self):\n",
    "        curr_time = time.perf_counter()\n",
    "        for img_t0 in self.batch_start_times:\n",
    "            time_in_queue = curr_time - img_t0\n",
    "            self.total_time_in_queue.value += time_in_queue\n",
    "            self.max_wait_time_in_queue.value = max(self.max_wait_time_in_queue.value, time_in_queue)\n",
    "        self.batch_start_times[:] = []\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )])\n",
    "with open('image_net_classes.txt') as file:\n",
    "    classes = [line.strip().split(', ')[1] for line in file.readlines()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def infer(model: Module, images, use_gpu=True, verbose=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if use_gpu:\n",
    "            model.cuda()\n",
    "        images_t = [transform(im) for im in images]\n",
    "        batch = torch.cat([tensor for tensor in [torch.unsqueeze(im_t, 0) for im_t in images_t]])\n",
    "        if use_gpu:\n",
    "            out = model(batch.cuda())\n",
    "        else:\n",
    "            out = model(batch)\n",
    "    if verbose:\n",
    "        for prediction in out:\n",
    "            prediction = prediction.cpu()\n",
    "            _, indices = torch.sort(prediction, descending=True)\n",
    "            percentages = [(torch.nn.functional.softmax(prediction, dim=0)[class_index] * 100).item() for class_index in\n",
    "                           indices[:5]]\n",
    "\n",
    "            logging.info(f'Rank\\tInferred class\\tProbability(%)')\n",
    "            for idx, class_index in enumerate(indices[:5]):\n",
    "                logging.info(f'#{idx}\\t\\t{classes[class_index]}\\t{percentages[idx]}')\n",
    "            logging.info('-----------------------------------------')\n",
    "\n",
    "\n",
    "def run_experiment(model_, input_images_):\n",
    "    t_0 = time.perf_counter()\n",
    "    infer(model_, input_images_, use_gpu=True)\n",
    "    return time.perf_counter() - t_0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def write_results(_file_name, _batch_size, _average, _average_util, _duration, _wait_time, _max_wait_time, _peak_average, _batch_average, _total_time, _inference_time):\n",
    "    file = open(f'results/{_file_name}', 'a')\n",
    "    logging.info(f'Batch Size\\tAverage Power(W)\\t\\tTime(s)\\t\\t\\t\\tEnergy(J)\\t\\t\\tAverage Wait Time(s)\\tMax Wait Time(s)\\tAverage Peak Power (W)\\tTotal time per image\\tInference time per image')\n",
    "    logging.info(f'{_batch_size if _batch_size > 0 else f\"Greedy ({_batch_average})\"}\\t\\t\\t{_average}\\t\\t{_duration}\\t{_average * _duration}\\t{_wait_time}\\t\\t{_max_wait_time}\\t\\t{_peak_average}')\n",
    "    file.write(f'{_batch_size if _batch_size > 0 else f\"Greedy ({_batch_average})\"},{_average},{_average_util},{_duration},{_average * _duration},{_wait_time},{_max_wait_time},{_peak_average},{_total_time},{_inference_time}\\n')\n",
    "    file.close()\n",
    "    logging.info(f'Results logged to: results/{file_name}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "batch_sizes = [16, 32, 64, 128,-1]\n",
    "frequency = 16 # 16, 32, 64, 128\n",
    "nr_of_requests = 8192\n",
    "models = [models.alexnet(pretrained=True), models.densenet121(pretrained=True), models.shufflenet_v2_x0_5(pretrained=True), models.vit_b_16(pretrained=True), models.convnext_base(pretrained=True), models.resnet50(pretrained=True), models.mobilenet_v2(pretrained=True), models.efficientnet_b7(pretrained=True)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WARMUP\n",
    "for _ in range(256):\n",
    "    batch = [Image.open('img/dog.jpg') for _ in range(32)]\n",
    "    run_experiment(models[0], batch)\n",
    "###\n",
    "\n",
    "for model in models:\n",
    "    file_name = f'{model.__class__.__name__}_f{frequency}'\n",
    "    file = open(f'results/{file_name}', 'a')\n",
    "    file.write(f'Batch Size,Average Power(W), Average Utilization(%),Time(s),Energy(J),Average Wait Time(s),Max Wait Time(s),Average Peak Power (W), Total time per image, Inference time per image\\n')\n",
    "    file.close()\n",
    "    for batch_size in batch_sizes:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        epsilon = 2 / frequency + 1\n",
    "        gpu_monitor = GPUMonitor(0.01)\n",
    "        img_count = 0\n",
    "        batches = []\n",
    "\n",
    "        rq = RequestQueue(f'inference_simulation_f{frequency}_{batch_size}', frequency, nr_of_requests)\n",
    "        t_0 = time.perf_counter()\n",
    "        while True:\n",
    "            try:\n",
    "                batch = [Image.open(rq.get_request(block=True, timeout=epsilon)) for _ in\n",
    "                         range(max(min(rq.queue.qsize(), 128), 1) if batch_size <= 0 else batch_size)]\n",
    "                t = run_experiment(model, batch)\n",
    "                rq.update_wait_times()\n",
    "                img_count += len(batch)\n",
    "                batches.append(len(batch))\n",
    "                # logging.info(f\"{100 * img_count / nr_of_requests}% last batch ({len(batch)}) took {t}s\")\n",
    "            except queue.Empty:\n",
    "                break\n",
    "\n",
    "        power = gpu_monitor.power_readings[:]\n",
    "        average = np.mean(power)\n",
    "        utilization = gpu_monitor.utilization_readings[:]\n",
    "        average_utilization = np.mean(utilization)\n",
    "        duration = time.perf_counter() - t_0 - epsilon # shouldn't it be placed right after the while loop?\n",
    "        inference_time = duration / nr_of_requests\n",
    "        wait_time = rq.total_time_in_queue.value / nr_of_requests\n",
    "        total_time = inference_time + wait_time\n",
    "        max_wait_time = rq.max_wait_time_in_queue.value\n",
    "        peak_average = np.mean(list(filter(lambda p: p > 65, power)))\n",
    "        average_batch_size = np.mean(batches)\n",
    "        gpu_monitor.plot_power()\n",
    "        gpu_monitor.plot_utilization()\n",
    "        write_results(file_name, batch_size, average, average_utilization, duration, wait_time, max_wait_time, peak_average, average_batch_size, total_time, inference_time)\n",
    "        rq.terminate()\n",
    "        gpu_monitor.terminate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
